"""
GuiXiaoXiRag FastAPIæœåŠ¡é…ç½®æ–‡ä»¶
ä¼˜åŒ–ç‰ˆæœ¬ - æ”¯æŒ .env æ–‡ä»¶å’Œç¯å¢ƒå˜é‡é…ç½®
"""
import os
from pathlib import Path
from typing import Optional, List
try:
    from pydantic_settings import BaseSettings
    from pydantic import Field
except ImportError:
    from pydantic import BaseSettings, Field


def get_project_root() -> Path:
    """è·å–é¡¹ç›®æ ¹ç›®å½•"""
    current_file = Path(__file__).resolve()
    # ä» server_new/common/config.py å‘ä¸Šä¸‰çº§åˆ°è¾¾é¡¹ç›®æ ¹ç›®å½•
    return current_file.parent.parent.parent


def find_env_file() -> Optional[Path]:
    """æŸ¥æ‰¾ .env æ–‡ä»¶ï¼ŒæŒ‰ä¼˜å…ˆçº§é¡ºåºæŸ¥æ‰¾"""
    project_root = get_project_root()
    possible_locations = [
        project_root / ".env",
        project_root / ".env.local",
        # project_root / "server_new" / ".env",
        Path.cwd() / ".env"
    ]

    for env_file in possible_locations:
        if env_file.exists():
            return env_file

    return None


class Settings(BaseSettings):
    """åº”ç”¨é…ç½® - æ”¯æŒä» .env æ–‡ä»¶å’Œç¯å¢ƒå˜é‡è¯»å–"""

    # åº”ç”¨ä¿¡æ¯
    app_name: str = Field(default="GuiXiaoXiRag FastAPI Service", description="åº”ç”¨åç§°")
    app_version: str = Field(default="2.0.0", description="åº”ç”¨ç‰ˆæœ¬")

    # æœåŠ¡é…ç½®
    host: str = Field(default="0.0.0.0", description="æœåŠ¡å™¨ä¸»æœºåœ°å€")
    port: int = Field(default=8002, description="æœåŠ¡å™¨ç«¯å£")
    debug: bool = Field(default=False, description="è°ƒè¯•æ¨¡å¼")
    workers: int = Field(default=1, description="å·¥ä½œè¿›ç¨‹æ•°")

    # GuiXiaoXiRagé…ç½®
    working_dir: str = Field(default="./knowledgeBase/default", description="çŸ¥è¯†åº“å·¥ä½œç›®å½•")

    # å¤§æ¨¡å‹é…ç½® - æ”¯æŒç”¨æˆ·è‡ªå®šä¹‰ï¼Œæœªé…ç½®æ—¶ä½¿ç”¨é»˜è®¤å€¼
    openai_api_base: str = Field(default="http://localhost:8100/v1", description="OpenAI API åŸºç¡€URL")
    openai_embedding_api_base: str = Field(default="http://localhost:8200/v1", description="OpenAI Embedding API åŸºç¡€URL")
    openai_chat_api_key: str = Field(default="your_api_key_here", description="OpenAI Chat API å¯†é’¥")
    openai_embedding_api_key: str = Field(default="your_api_key_here", description="OpenAI Embedding API å¯†é’¥")
    openai_chat_model: str = Field(default="qwen14b", description="OpenAI Chat æ¨¡å‹")
    openai_embedding_model: str = Field(default="embedding_qwen", description="OpenAI Embedding æ¨¡å‹")

    # LLMæ„å›¾è¯†åˆ«é…ç½®
    llm_enabled: bool = Field(default=True, description="å¯ç”¨LLMæ„å›¾è¯†åˆ«")
    llm_provider: str = Field(default="openai", description="LLMæä¾›å•†: openai, azure, ollama, custom")
    llm_temperature: float = Field(default=0.1, description="LLMæ¸©åº¦å‚æ•°")
    llm_max_tokens: int = Field(default=2048, description="LLMæœ€å¤§tokenæ•°")
    llm_timeout: int = Field(default=30, description="LLMè¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)")

    # Embeddingé…ç½®
    embedding_enabled: bool = Field(default=True, description="å¯ç”¨EmbeddingæœåŠ¡")
    embedding_provider: str = Field(default="openai", description="Embeddingæä¾›å•†: openai, azure, ollama, custom")
    embedding_timeout: int = Field(default=30, description="Embeddingè¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)")

    # Reranké…ç½®
    rerank_enabled: bool = Field(default=False, description="å¯ç”¨RerankæœåŠ¡")
    rerank_provider: str = Field(default="openai", description="Rerankæä¾›å•†: openai, azure, custom")
    rerank_model: str = Field(default="rerank-multilingual-v3.0", description="Rerankæ¨¡å‹")
    rerank_timeout: int = Field(default=30, description="Rerankè¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)")
    rerank_top_k: int = Field(default=10, description="Rerankè¿”å›çš„top-kç»“æœæ•°é‡")

    # å¯é€‰çš„è‡ªå®šä¹‰é…ç½®
    custom_llm_provider: Optional[str] = Field(default=None, description="è‡ªå®šä¹‰LLMæä¾›å•†")
    custom_embedding_provider: Optional[str] = Field(default=None, description="è‡ªå®šä¹‰Embeddingæä¾›å•†")
    azure_api_version: Optional[str] = Field(default="2023-12-01-preview", description="Azure APIç‰ˆæœ¬")
    azure_deployment_name: Optional[str] = Field(default="gpt-35-turbo", description="Azureéƒ¨ç½²åç§°")

    # Ollama é…ç½®
    ollama_base_url: str = Field(default="http://localhost:11434", description="Ollama åŸºç¡€URL")
    ollama_chat_model: str = Field(default="llama2", description="Ollama Chat æ¨¡å‹")
    ollama_embedding_model: Optional[str] = Field(default=None, description="Ollama Embedding æ¨¡å‹")

    # Embeddingé…ç½®
    embedding_dim: int = Field(default=2560, description="å‘é‡ç»´åº¦")
    max_token_size: int = Field(default=8192, description="æœ€å¤§tokenæ•°é‡")

    # æ—¥å¿—é…ç½®
    log_level: str = Field(default="INFO", description="æ—¥å¿—çº§åˆ«")
    log_dir: str = Field(default="./logs", description="æ—¥å¿—ç›®å½•")

    # æ–‡ä»¶ä¸Šä¼ é…ç½®
    max_file_size: int = Field(default=50 * 1024 * 1024, description="æœ€å¤§æ–‡ä»¶å¤§å°(å­—èŠ‚)")
    allowed_file_types: List[str] = Field(
        default=[".txt", ".pdf", ".docx", ".doc", ".md", ".json", ".xml", ".csv"],
        description="å…è®¸çš„æ–‡ä»¶ç±»å‹"
    )
    upload_dir: str = Field(default="./uploads", description="ä¸Šä¼ ç›®å½•")

    # Streamlité…ç½®
    streamlit_host: str = Field(default="0.0.0.0", description="Streamlitä¸»æœºåœ°å€")
    streamlit_port: int = Field(default=8501, description="Streamlitç«¯å£")
    streamlit_api_url: str = Field(default="http://localhost:8002", description="Streamlit API URL")
    streamlit_timeout: int = Field(default=120, description="Streamlitè¶…æ—¶æ—¶é—´")

    # æ€§èƒ½é…ç½®
    enable_cache: bool = Field(default=True, description="å¯ç”¨ç¼“å­˜")
    cache_ttl: int = Field(default=3600, description="ç¼“å­˜TTL(ç§’)")
    max_concurrent_requests: int = Field(default=100, description="æœ€å¤§å¹¶å‘è¯·æ±‚æ•°")

    # å®‰å…¨é…ç½®
    cors_origins: List[str] = Field(default=["*"], description="CORSå…è®¸çš„æº")
    cors_methods: List[str] = Field(default=["*"], description="CORSå…è®¸çš„æ–¹æ³•")
    cors_headers: List[str] = Field(default=["*"], description="CORSå…è®¸çš„å¤´éƒ¨")

    class Config:
        # åŠ¨æ€æŸ¥æ‰¾ .env æ–‡ä»¶
        env_file = find_env_file()
        env_file_encoding = "utf-8"
        case_sensitive = False
        # å¿½ç•¥é¢å¤–çš„å­—æ®µï¼Œé¿å…éªŒè¯é”™è¯¯
        extra = "ignore"
        # æ”¯æŒä»å¤šä¸ªä½ç½®è¯»å–ç¯å¢ƒæ–‡ä»¶
        env_files = [
            get_project_root() / ".env",
            get_project_root() / ".env.local",
            # get_project_root() / "server_new" / ".env",
            Path.cwd() / ".env"
        ]


# å…¨å±€é…ç½®å®ä¾‹
def create_settings() -> Settings:
    """åˆ›å»ºé…ç½®å®ä¾‹å¹¶æ˜¾ç¤ºåŠ è½½ä¿¡æ¯"""
    env_file = find_env_file()
    if env_file:
        print(f"âœ… æ‰¾åˆ°é…ç½®æ–‡ä»¶: {env_file}")
    else:
        print("âš ï¸  æœªæ‰¾åˆ° .env æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        print(f"ğŸ’¡ å»ºè®®åœ¨é¡¹ç›®æ ¹ç›®å½• {get_project_root()} åˆ›å»º .env æ–‡ä»¶")

    return Settings()

settings = create_settings()


def ensure_directories():
    """ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨"""
    directories = [
        settings.working_dir,
        settings.log_dir,
        settings.upload_dir,
        Path(settings.working_dir).parent,  # knowledgeBaseç›®å½•
    ]

    for directory in directories:
        os.makedirs(directory, exist_ok=True)


def validate_config():
    """éªŒè¯é…ç½®çš„æœ‰æ•ˆæ€§"""
    errors = []
    warnings = []

    # éªŒè¯ç«¯å£èŒƒå›´
    if not (1 <= settings.port <= 65535):
        errors.append(f"ç«¯å£å·æ— æ•ˆ: {settings.port}")

    if not (1 <= settings.streamlit_port <= 65535):
        errors.append(f"Streamlitç«¯å£å·æ— æ•ˆ: {settings.streamlit_port}")

    # æ™ºèƒ½éªŒè¯APIé…ç½®
    if settings.openai_chat_api_key == "your_api_key_here":
        warnings.append("ä½¿ç”¨é»˜è®¤LLM APIå¯†é’¥ï¼Œè¯·åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è®¾ç½®æœ‰æ•ˆçš„APIå¯†é’¥")

    if settings.openai_embedding_api_key == "your_api_key_here":
        warnings.append("ä½¿ç”¨é»˜è®¤Embedding APIå¯†é’¥ï¼Œè¯·åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è®¾ç½®æœ‰æ•ˆçš„APIå¯†é’¥")

    # éªŒè¯APIåŸºç¡€URLæ ¼å¼
    if not settings.openai_api_base.startswith(('http://', 'https://')):
        errors.append(f"LLM APIåŸºç¡€URLæ ¼å¼æ— æ•ˆ: {settings.openai_api_base}")

    if not settings.openai_embedding_api_base.startswith(('http://', 'https://')):
        errors.append(f"Embedding APIåŸºç¡€URLæ ¼å¼æ— æ•ˆ: {settings.openai_embedding_api_base}")

    # éªŒè¯ç›®å½•è·¯å¾„
    try:
        Path(settings.working_dir).resolve()
        Path(settings.log_dir).resolve()
        Path(settings.upload_dir).resolve()
    except Exception as e:
        errors.append(f"ç›®å½•è·¯å¾„æ— æ•ˆ: {e}")

    # éªŒè¯æ–‡ä»¶å¤§å°
    if settings.max_file_size <= 0:
        errors.append("æ–‡ä»¶å¤§å°é™åˆ¶å¿…é¡»å¤§äº0")

    # è¾“å‡ºéªŒè¯ç»“æœ
    if warnings:
        print("âš ï¸  é…ç½®è­¦å‘Š:")
        for warning in warnings:
            print(f"   - {warning}")

    if errors:
        print("âŒ é…ç½®é”™è¯¯:")
        for error in errors:
            print(f"   - {error}")

    return len(errors) == 0


def get_effective_config():
    """è·å–æœ‰æ•ˆçš„é…ç½®ä¿¡æ¯ï¼Œå¤„ç†ç”¨æˆ·è‡ªå®šä¹‰å’Œé»˜è®¤å€¼"""
    config = {
        "app_name": settings.app_name,
        "version": settings.app_version,
        "host": settings.host,
        "port": settings.port,
        "debug": settings.debug,
        "working_dir": settings.working_dir,
        "log_level": settings.log_level,

        # LLMé…ç½®
        "llm": {
            "api_base": settings.openai_api_base,
            "api_key": "***" if settings.openai_chat_api_key != "your_api_key_here" else "æœªé…ç½®",
            "model": settings.openai_chat_model,
            "provider": settings.custom_llm_provider or "openai",
        },

        # Embeddingé…ç½®
        "embedding": {
            "api_base": settings.openai_embedding_api_base,
            "api_key": "***" if settings.openai_embedding_api_key != "your_api_key_here" else "æœªé…ç½®",
            "model": settings.openai_embedding_model,
            "dim": settings.embedding_dim,
            "provider": settings.custom_embedding_provider or "openai",
        },

        # å…¶ä»–é…ç½®
        "max_file_size_mb": settings.max_file_size // (1024 * 1024),
        "streamlit_port": settings.streamlit_port,
        "max_token_size": settings.max_token_size,
    }

    # æ·»åŠ Azureç‰¹å®šé…ç½®ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
    if settings.azure_api_version:
        config["azure"] = {
            "api_version": settings.azure_api_version,
            "deployment_name": settings.azure_deployment_name,
        }

    return config


def get_llm_config():
    """è·å–LLMé…ç½®"""
    provider = settings.llm_provider.lower()

    base_config = {
        "temperature": settings.llm_temperature,
        "max_tokens": settings.llm_max_tokens,
        "timeout": settings.llm_timeout,
    }

    if provider == "openai":
        return {
            **base_config,
            "api_base": settings.openai_api_base,
            "api_key": settings.openai_chat_api_key,
            "model": settings.openai_chat_model,
        }
    elif provider == "azure":
        return {
            **base_config,
            "api_base": settings.openai_api_base,
            "api_key": settings.openai_chat_api_key,
            "api_version": settings.azure_api_version,
            "deployment_name": settings.azure_deployment_name,
        }
    elif provider == "ollama":
        return {
            **base_config,
            "api_base": settings.ollama_base_url,
            "model": settings.ollama_chat_model,
        }
    elif provider == "custom":
        return {
            **base_config,
            "api_base": settings.openai_api_base,
            "api_key": settings.openai_chat_api_key,
            "model": settings.openai_chat_model,
        }
    else:
        return base_config


def get_embedding_config():
    """è·å–Embeddingé…ç½®"""
    provider = settings.embedding_provider.lower()

    base_config = {
        "timeout": settings.embedding_timeout,
    }

    if provider == "openai":
        return {
            **base_config,
            "api_base": settings.openai_embedding_api_base,
            "api_key": settings.openai_embedding_api_key,
            "model": settings.openai_embedding_model,
        }
    elif provider == "azure":
        return {
            **base_config,
            "api_base": settings.openai_embedding_api_base,
            "api_key": settings.openai_embedding_api_key,
            "api_version": settings.azure_api_version,
            "deployment_name": settings.openai_embedding_model,
        }
    elif provider == "ollama":
        return {
            **base_config,
            "api_base": settings.ollama_base_url,
            "model": settings.ollama_embedding_model or "nomic-embed-text",
        }
    elif provider == "custom":
        return {
            **base_config,
            "api_base": settings.openai_embedding_api_base,
            "api_key": settings.openai_embedding_api_key,
            "model": settings.openai_embedding_model,
        }
    else:
        return base_config


def get_rerank_config():
    """è·å–Reranké…ç½®"""
    provider = settings.rerank_provider.lower()

    base_config = {
        "timeout": settings.rerank_timeout,
        "top_k": settings.rerank_top_k,
    }

    if provider == "openai":
        return {
            **base_config,
            "api_base": settings.openai_api_base,
            "api_key": settings.openai_chat_api_key,
            "model": settings.rerank_model,
        }
    elif provider == "azure":
        return {
            **base_config,
            "api_base": settings.openai_api_base,
            "api_key": settings.openai_chat_api_key,
            "api_version": settings.azure_api_version,
            "deployment_name": settings.rerank_model,
        }
    elif provider == "custom":
        return {
            **base_config,
            "api_base": settings.openai_api_base,
            "api_key": settings.openai_chat_api_key,
            "model": settings.rerank_model,
        }
    else:
        return base_config


def get_config_summary():
    """è·å–é…ç½®æ‘˜è¦ä¿¡æ¯"""
    effective_config = get_effective_config()
    return {
        "app_name": effective_config["app_name"],
        "version": effective_config["version"],
        "host": effective_config["host"],
        "port": effective_config["port"],
        "debug": effective_config["debug"],
        "working_dir": effective_config["working_dir"],
        "log_level": effective_config["log_level"],
        "openai_chat_model": effective_config["llm"]["model"],
        "openai_embedding_model": effective_config["embedding"]["model"],
        "embedding_dim": effective_config["embedding"]["dim"],
        "max_file_size_mb": effective_config["max_file_size_mb"],
        "streamlit_port": effective_config["streamlit_port"],
        "llm_enabled": settings.llm_enabled,
        "llm_provider": settings.llm_provider,
    }


# åˆå§‹åŒ–ç›®å½•
ensure_directories()

# å¯¼å‡ºå¸¸ç”¨é…ç½®
__all__ = [
    "settings",
    "ensure_directories",
    "validate_config",
    "get_config_summary",
    "get_effective_config",
    "get_llm_config",
    "get_embedding_config",
    "get_rerank_config",
    "get_project_root"
]
