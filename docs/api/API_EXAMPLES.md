# GuiXiaoXiRag FastAPI API è°ƒç”¨ç¤ºä¾‹

## ğŸ“‹ ç›®å½•

- [åŸºç¡€ç¤ºä¾‹](#åŸºç¡€ç¤ºä¾‹)
- [Pythonå®¢æˆ·ç«¯ç¤ºä¾‹](#pythonå®¢æˆ·ç«¯ç¤ºä¾‹)
- [JavaScriptç¤ºä¾‹](#javascriptç¤ºä¾‹)
- [cURLå‘½ä»¤ç¤ºä¾‹](#curlå‘½ä»¤ç¤ºä¾‹)
- [æ‰¹é‡æ“ä½œç¤ºä¾‹](#æ‰¹é‡æ“ä½œç¤ºä¾‹)
- [é«˜çº§åŠŸèƒ½ç¤ºä¾‹](#é«˜çº§åŠŸèƒ½ç¤ºä¾‹)
- [é”™è¯¯å¤„ç†ç¤ºä¾‹](#é”™è¯¯å¤„ç†ç¤ºä¾‹)
- [æ€§èƒ½ä¼˜åŒ–ç¤ºä¾‹](#æ€§èƒ½ä¼˜åŒ–ç¤ºä¾‹)

## åŸºç¡€ç¤ºä¾‹

### å¥åº·æ£€æŸ¥
```bash
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
curl http://localhost:8002/health

# å“åº”ç¤ºä¾‹
{
  "status": "healthy",
  "timestamp": "2024-01-01T12:00:00Z",
  "version": "1.0.0",
  "uptime": "2 days, 3 hours, 45 minutes"
}
```

### æ’å…¥å•ä¸ªæ–‡æ¡£
```bash
curl -X POST "http://localhost:8002/insert/text" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚",
    "doc_id": "ai_intro_001",
    "metadata": {
      "source": "manual_input",
      "category": "technology",
      "author": "system"
    }
  }'

# å“åº”ç¤ºä¾‹
{
  "success": true,
  "message": "æ–‡æ¡£æ’å…¥æˆåŠŸ",
  "data": {
    "doc_id": "ai_intro_001",
    "chunks_created": 1,
    "processing_time": 2.34
  }
}
```

### åŸºç¡€æŸ¥è¯¢
```bash
curl -X POST "http://localhost:8002/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
    "mode": "hybrid",
    "top_k": 5
  }'

# å“åº”ç¤ºä¾‹
{
  "success": true,
  "data": {
    "answer": "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯...",
    "sources": [
      {
        "doc_id": "ai_intro_001",
        "chunk_id": "chunk_001",
        "score": 0.95,
        "content": "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯..."
      }
    ],
    "query_time": 1.23,
    "mode": "hybrid"
  }
}
```

## Pythonå®¢æˆ·ç«¯ç¤ºä¾‹

### åŸºç¡€å®¢æˆ·ç«¯ç±»
```python
import requests
import json
from typing import Dict, List, Optional

class GuiXiaoXiRagClient:
    def __init__(self, base_url: str = "http://localhost:8002"):
        self.base_url = base_url
        self.session = requests.Session()
    
    def health_check(self) -> Dict:
        """æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€"""
        response = self.session.get(f"{self.base_url}/health")
        response.raise_for_status()
        return response.json()
    
    def insert_text(self, text: str, doc_id: Optional[str] = None, 
                   metadata: Optional[Dict] = None) -> Dict:
        """æ’å…¥æ–‡æœ¬æ–‡æ¡£"""
        data = {"text": text}
        if doc_id:
            data["doc_id"] = doc_id
        if metadata:
            data["metadata"] = metadata
        
        response = self.session.post(
            f"{self.base_url}/insert/text",
            json=data
        )
        response.raise_for_status()
        return response.json()
    
    def query(self, query: str, mode: str = "hybrid", 
             top_k: int = 10, **kwargs) -> Dict:
        """æŸ¥è¯¢çŸ¥è¯†åº“"""
        data = {
            "query": query,
            "mode": mode,
            "top_k": top_k,
            **kwargs
        }
        
        response = self.session.post(
            f"{self.base_url}/query",
            json=data
        )
        response.raise_for_status()
        return response.json()
    
    def upload_file(self, file_path: str) -> Dict:
        """ä¸Šä¼ æ–‡ä»¶"""
        with open(file_path, 'rb') as f:
            files = {'file': f}
            response = self.session.post(
                f"{self.base_url}/insert/file",
                files=files
            )
        response.raise_for_status()
        return response.json()

# ä½¿ç”¨ç¤ºä¾‹
client = GuiXiaoXiRagClient()

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
health = client.health_check()
print(f"Service status: {health['status']}")

# æ’å…¥æ–‡æ¡£
result = client.insert_text(
    text="æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹ã€‚",
    doc_id="dl_intro_001",
    metadata={"category": "AI", "level": "beginner"}
)
print(f"Document inserted: {result['data']['doc_id']}")

# æŸ¥è¯¢
answer = client.query("ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ", mode="hybrid", top_k=5)
print(f"Answer: {answer['data']['answer']}")
```

### æ‰¹é‡æ“ä½œç¤ºä¾‹
```python
def batch_insert_texts(client: GuiXiaoXiRagClient, texts: List[str]) -> List[Dict]:
    """æ‰¹é‡æ’å…¥æ–‡æœ¬"""
    results = []
    
    # ä½¿ç”¨æ‰¹é‡æ¥å£
    response = client.session.post(
        f"{client.base_url}/insert/texts",
        json={"texts": texts}
    )
    response.raise_for_status()
    return response.json()

def batch_query(client: GuiXiaoXiRagClient, queries: List[str]) -> List[Dict]:
    """æ‰¹é‡æŸ¥è¯¢"""
    response = client.session.post(
        f"{client.base_url}/query/batch",
        json={"queries": queries, "mode": "hybrid"}
    )
    response.raise_for_status()
    return response.json()

# ä½¿ç”¨ç¤ºä¾‹
texts = [
    "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚",
    "è‡ªç„¶è¯­è¨€å¤„ç†ç”¨äºè®©è®¡ç®—æœºç†è§£äººç±»è¯­è¨€ã€‚",
    "è®¡ç®—æœºè§†è§‰è®©æœºå™¨èƒ½å¤Ÿç†è§£å’Œåˆ†æå›¾åƒã€‚"
]

# æ‰¹é‡æ’å…¥
batch_result = batch_insert_texts(client, texts)
print(f"Batch insert completed: {len(batch_result['data']['results'])} documents")

# æ‰¹é‡æŸ¥è¯¢
queries = ["ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ", "NLPçš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ"]
batch_answers = batch_query(client, queries)
for i, answer in enumerate(batch_answers['data']['results']):
    print(f"Query {i+1}: {answer['answer']}")
```

### å¼‚æ­¥å®¢æˆ·ç«¯ç¤ºä¾‹
```python
import asyncio
import aiohttp
from typing import Dict, List

class AsyncGuiXiaoXiRagClient:
    def __init__(self, base_url: str = "http://localhost:8002"):
        self.base_url = base_url
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.session.close()
    
    async def query(self, query: str, mode: str = "hybrid") -> Dict:
        """å¼‚æ­¥æŸ¥è¯¢"""
        async with self.session.post(
            f"{self.base_url}/query",
            json={"query": query, "mode": mode}
        ) as response:
            response.raise_for_status()
            return await response.json()
    
    async def concurrent_queries(self, queries: List[str]) -> List[Dict]:
        """å¹¶å‘æŸ¥è¯¢"""
        tasks = [self.query(query) for query in queries]
        return await asyncio.gather(*tasks)

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    async with AsyncGuiXiaoXiRagClient() as client:
        queries = [
            "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
            "æœºå™¨å­¦ä¹ çš„åº”ç”¨æœ‰å“ªäº›ï¼Ÿ",
            "æ·±åº¦å­¦ä¹ å’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ çš„åŒºåˆ«ï¼Ÿ"
        ]
        
        # å¹¶å‘æŸ¥è¯¢
        results = await client.concurrent_queries(queries)
        
        for i, result in enumerate(results):
            print(f"Query {i+1}: {result['data']['answer'][:100]}...")

# è¿è¡Œå¼‚æ­¥ç¤ºä¾‹
# asyncio.run(main())
```

## JavaScriptç¤ºä¾‹

### åŸºç¡€JavaScriptå®¢æˆ·ç«¯
```javascript
class GuiXiaoXiRagClient {
    constructor(baseUrl = 'http://localhost:8002') {
        this.baseUrl = baseUrl;
    }
    
    async healthCheck() {
        const response = await fetch(`${this.baseUrl}/health`);
        if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`);
        }
        return await response.json();
    }
    
    async insertText(text, docId = null, metadata = null) {
        const data = { text };
        if (docId) data.doc_id = docId;
        if (metadata) data.metadata = metadata;
        
        const response = await fetch(`${this.baseUrl}/insert/text`, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
            },
            body: JSON.stringify(data)
        });
        
        if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`);
        }
        return await response.json();
    }
    
    async query(query, mode = 'hybrid', topK = 10) {
        const response = await fetch(`${this.baseUrl}/query`, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
            },
            body: JSON.stringify({
                query: query,
                mode: mode,
                top_k: topK
            })
        });
        
        if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`);
        }
        return await response.json();
    }
    
    async uploadFile(file) {
        const formData = new FormData();
        formData.append('file', file);
        
        const response = await fetch(`${this.baseUrl}/insert/file`, {
            method: 'POST',
            body: formData
        });
        
        if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`);
        }
        return await response.json();
    }
}

// ä½¿ç”¨ç¤ºä¾‹
const client = new GuiXiaoXiRagClient();

// æ£€æŸ¥æœåŠ¡çŠ¶æ€
client.healthCheck()
    .then(health => console.log('Service status:', health.status))
    .catch(error => console.error('Health check failed:', error));

// æ’å…¥æ–‡æ¡£
client.insertText(
    'åŒºå—é“¾æ˜¯ä¸€ç§åˆ†å¸ƒå¼è´¦æœ¬æŠ€æœ¯ï¼Œå…·æœ‰å»ä¸­å¿ƒåŒ–ã€ä¸å¯ç¯¡æ”¹çš„ç‰¹ç‚¹ã€‚',
    'blockchain_001',
    { category: 'technology', level: 'intermediate' }
)
.then(result => console.log('Document inserted:', result.data.doc_id))
.catch(error => console.error('Insert failed:', error));

// æŸ¥è¯¢
client.query('ä»€ä¹ˆæ˜¯åŒºå—é“¾ï¼Ÿ', 'hybrid', 5)
    .then(result => console.log('Answer:', result.data.answer))
    .catch(error => console.error('Query failed:', error));
```

### Reactç»„ä»¶ç¤ºä¾‹
```jsx
import React, { useState, useEffect } from 'react';

const GuiXiaoXiRagInterface = () => {
    const [client] = useState(new GuiXiaoXiRagClient());
    const [query, setQuery] = useState('');
    const [answer, setAnswer] = useState('');
    const [loading, setLoading] = useState(false);
    const [health, setHealth] = useState(null);
    
    useEffect(() => {
        // æ£€æŸ¥æœåŠ¡çŠ¶æ€
        client.healthCheck()
            .then(setHealth)
            .catch(console.error);
    }, [client]);
    
    const handleQuery = async () => {
        if (!query.trim()) return;
        
        setLoading(true);
        try {
            const result = await client.query(query);
            setAnswer(result.data.answer);
        } catch (error) {
            console.error('Query failed:', error);
            setAnswer('æŸ¥è¯¢å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•ã€‚');
        } finally {
            setLoading(false);
        }
    };
    
    const handleFileUpload = async (event) => {
        const file = event.target.files[0];
        if (!file) return;
        
        try {
            const result = await client.uploadFile(file);
            alert(`æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: ${result.data.doc_id}`);
        } catch (error) {
            console.error('Upload failed:', error);
            alert('æ–‡ä»¶ä¸Šä¼ å¤±è´¥');
        }
    };
    
    return (
        <div className="guixiaoxirag-interface">
            <div className="status">
                çŠ¶æ€: {health ? health.status : 'æ£€æŸ¥ä¸­...'}
            </div>
            
            <div className="upload-section">
                <input 
                    type="file" 
                    onChange={handleFileUpload}
                    accept=".txt,.pdf,.docx,.md"
                />
            </div>
            
            <div className="query-section">
                <input
                    type="text"
                    value={query}
                    onChange={(e) => setQuery(e.target.value)}
                    placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."
                    onKeyPress={(e) => e.key === 'Enter' && handleQuery()}
                />
                <button onClick={handleQuery} disabled={loading}>
                    {loading ? 'æŸ¥è¯¢ä¸­...' : 'æŸ¥è¯¢'}
                </button>
            </div>
            
            {answer && (
                <div className="answer-section">
                    <h3>å›ç­”:</h3>
                    <p>{answer}</p>
                </div>
            )}
        </div>
    );
};

export default GuiXiaoXiRagInterface;
```

## cURLå‘½ä»¤ç¤ºä¾‹

### å®Œæ•´çš„å·¥ä½œæµç¨‹
```bash
#!/bin/bash

# 1. æ£€æŸ¥æœåŠ¡çŠ¶æ€
echo "=== æ£€æŸ¥æœåŠ¡çŠ¶æ€ ==="
curl -s http://localhost:8002/health | jq '.'

# 2. æ’å…¥æµ‹è¯•æ–‡æ¡£
echo -e "\n=== æ’å…¥æµ‹è¯•æ–‡æ¡£ ==="
curl -X POST "http://localhost:8002/insert/text" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "äº‘è®¡ç®—æ˜¯ä¸€ç§é€šè¿‡äº’è”ç½‘æä¾›è®¡ç®—æœåŠ¡çš„æ¨¡å¼ï¼ŒåŒ…æ‹¬æœåŠ¡å™¨ã€å­˜å‚¨ã€æ•°æ®åº“ã€ç½‘ç»œã€è½¯ä»¶ç­‰ã€‚",
    "doc_id": "cloud_computing_001",
    "metadata": {"category": "technology", "level": "basic"}
  }' | jq '.'

# 3. æ‰¹é‡æ’å…¥æ–‡æ¡£
echo -e "\n=== æ‰¹é‡æ’å…¥æ–‡æ¡£ ==="
curl -X POST "http://localhost:8002/insert/texts" \
  -H "Content-Type: application/json" \
  -d '{
    "texts": [
      "å¤§æ•°æ®æ˜¯æŒ‡æ— æ³•åœ¨ä¸€å®šæ—¶é—´èŒƒå›´å†…ç”¨å¸¸è§„è½¯ä»¶å·¥å…·è¿›è¡Œæ•æ‰ã€ç®¡ç†å’Œå¤„ç†çš„æ•°æ®é›†åˆã€‚",
      "ç‰©è”ç½‘æ˜¯æŒ‡é€šè¿‡ä¿¡æ¯ä¼ æ„Ÿè®¾å¤‡ï¼Œå°†ä»»ä½•ç‰©å“ä¸äº’è”ç½‘ç›¸è¿æ¥ï¼Œè¿›è¡Œä¿¡æ¯äº¤æ¢å’Œé€šä¿¡ã€‚"
    ],
    "doc_ids": ["bigdata_001", "iot_001"]
  }' | jq '.'

# 4. ä¸Šä¼ æ–‡ä»¶
echo -e "\n=== ä¸Šä¼ æ–‡ä»¶ ==="
curl -X POST "http://localhost:8002/insert/file" \
  -F "file=@example.pdf" | jq '.'

# 5. ä¸åŒæ¨¡å¼çš„æŸ¥è¯¢
echo -e "\n=== æ··åˆæ¨¡å¼æŸ¥è¯¢ ==="
curl -X POST "http://localhost:8002/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "ä»€ä¹ˆæ˜¯äº‘è®¡ç®—ï¼Ÿ",
    "mode": "hybrid",
    "top_k": 5
  }' | jq '.data.answer'

echo -e "\n=== æœ¬åœ°æ¨¡å¼æŸ¥è¯¢ ==="
curl -X POST "http://localhost:8002/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "å¤§æ•°æ®çš„ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
    "mode": "local",
    "top_k": 3
  }' | jq '.data.answer'

# 6. çŸ¥è¯†åº“ç®¡ç†
echo -e "\n=== åˆ›å»ºçŸ¥è¯†åº“ ==="
curl -X POST "http://localhost:8002/knowledge-bases" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "tech_kb",
    "description": "æŠ€æœ¯çŸ¥è¯†åº“"
  }' | jq '.'

echo -e "\n=== æŸ¥çœ‹çŸ¥è¯†åº“åˆ—è¡¨ ==="
curl -s http://localhost:8002/knowledge-bases | jq '.'

# 7. è·å–ç³»ç»Ÿç»Ÿè®¡
echo -e "\n=== ç³»ç»Ÿç»Ÿè®¡ ==="
curl -s http://localhost:8002/knowledge-graph/stats | jq '.'

# 8. è·å–æ€§èƒ½æŒ‡æ ‡
echo -e "\n=== æ€§èƒ½æŒ‡æ ‡ ==="
curl -s http://localhost:8002/metrics | jq '.'
```

### é«˜çº§æŸ¥è¯¢ç¤ºä¾‹
```bash
# æµå¼æŸ¥è¯¢
curl -X POST "http://localhost:8002/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "è§£é‡Šæœºå™¨å­¦ä¹ çš„å·¥ä½œåŸç†",
    "mode": "global",
    "stream": true,
    "max_tokens": 1000
  }' --no-buffer

# å¸¦ä¸Šä¸‹æ–‡çš„æŸ¥è¯¢
curl -X POST "http://localhost:8002/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "å®ƒçš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ",
    "mode": "hybrid",
    "context": "æˆ‘ä»¬åˆšæ‰è®¨è®ºäº†äº‘è®¡ç®—çš„æ¦‚å¿µ",
    "top_k": 5
  }' | jq '.'

# å¤šè¯­è¨€æŸ¥è¯¢
curl -X POST "http://localhost:8002/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is artificial intelligence?",
    "mode": "hybrid",
    "language": "English",
    "response_language": "ä¸­æ–‡"
  }' | jq '.'
```

## æ‰¹é‡æ“ä½œç¤ºä¾‹

### Pythonæ‰¹é‡å¤„ç†è„šæœ¬
```python
import os
import time
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

def process_directory(client: GuiXiaoXiRagClient, directory_path: str):
    """æ‰¹é‡å¤„ç†ç›®å½•ä¸­çš„æ–‡ä»¶"""
    directory = Path(directory_path)
    supported_extensions = {'.txt', '.md', '.pdf', '.docx', '.doc'}
    
    files_to_process = [
        f for f in directory.rglob('*') 
        if f.is_file() and f.suffix.lower() in supported_extensions
    ]
    
    print(f"Found {len(files_to_process)} files to process")
    
    def upload_file(file_path):
        try:
            result = client.upload_file(str(file_path))
            return {"file": file_path.name, "success": True, "result": result}
        except Exception as e:
            return {"file": file_path.name, "success": False, "error": str(e)}
    
    # å¹¶å‘ä¸Šä¼ æ–‡ä»¶
    with ThreadPoolExecutor(max_workers=5) as executor:
        future_to_file = {
            executor.submit(upload_file, file_path): file_path 
            for file_path in files_to_process
        }
        
        results = []
        for future in as_completed(future_to_file):
            result = future.result()
            results.append(result)
            
            if result["success"]:
                print(f"âœ… {result['file']} uploaded successfully")
            else:
                print(f"âŒ {result['file']} failed: {result['error']}")
    
    return results

# ä½¿ç”¨ç¤ºä¾‹
client = GuiXiaoXiRagClient()
results = process_directory(client, "./documents")

success_count = sum(1 for r in results if r["success"])
print(f"\nProcessing completed: {success_count}/{len(results)} files successful")
```

### æ‰¹é‡æŸ¥è¯¢å’Œåˆ†æ
```python
def analyze_queries(client: GuiXiaoXiRagClient, queries: List[str]):
    """æ‰¹é‡æŸ¥è¯¢å¹¶åˆ†æç»“æœ"""
    results = []
    
    for i, query in enumerate(queries):
        print(f"Processing query {i+1}/{len(queries)}: {query[:50]}...")
        
        try:
            # å°è¯•ä¸åŒçš„æŸ¥è¯¢æ¨¡å¼
            modes = ["hybrid", "local", "global"]
            query_results = {}
            
            for mode in modes:
                start_time = time.time()
                result = client.query(query, mode=mode, top_k=5)
                end_time = time.time()
                
                query_results[mode] = {
                    "answer": result["data"]["answer"],
                    "sources_count": len(result["data"]["sources"]),
                    "response_time": end_time - start_time
                }
            
            results.append({
                "query": query,
                "results": query_results
            })
            
        except Exception as e:
            print(f"Error processing query: {e}")
            results.append({
                "query": query,
                "error": str(e)
            })
    
    return results

# åˆ†ææŸ¥è¯¢æ€§èƒ½
queries = [
    "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
    "æœºå™¨å­¦ä¹ çš„ä¸»è¦ç®—æ³•æœ‰å“ªäº›ï¼Ÿ",
    "æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ä¸­çš„åº”ç”¨",
    "è‡ªç„¶è¯­è¨€å¤„ç†çš„å‘å±•å†ç¨‹",
    "äº‘è®¡ç®—çš„å®‰å…¨æ€§å¦‚ä½•ä¿éšœï¼Ÿ"
]

analysis_results = analyze_queries(client, queries)

# ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
for result in analysis_results:
    if "error" not in result:
        print(f"\nQuery: {result['query']}")
        for mode, data in result['results'].items():
            print(f"  {mode}: {data['response_time']:.2f}s, {data['sources_count']} sources")
```

## é”™è¯¯å¤„ç†ç¤ºä¾‹

### å®Œæ•´çš„é”™è¯¯å¤„ç†
```python
import logging
from requests.exceptions import RequestException, Timeout, ConnectionError

class GuiXiaoXiRagClientWithErrorHandling(GuiXiaoXiRagClient):
    def __init__(self, base_url: str = "http://localhost:8002", 
                 timeout: int = 30, max_retries: int = 3):
        super().__init__(base_url)
        self.timeout = timeout
        self.max_retries = max_retries
        self.logger = logging.getLogger(__name__)
    
    def _make_request(self, method: str, endpoint: str, **kwargs):
        """å¸¦é‡è¯•æœºåˆ¶çš„è¯·æ±‚æ–¹æ³•"""
        url = f"{self.base_url}{endpoint}"
        
        for attempt in range(self.max_retries):
            try:
                response = self.session.request(
                    method, url, timeout=self.timeout, **kwargs
                )
                response.raise_for_status()
                return response.json()
                
            except Timeout:
                self.logger.warning(f"Request timeout (attempt {attempt + 1})")
                if attempt == self.max_retries - 1:
                    raise
                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                
            except ConnectionError:
                self.logger.warning(f"Connection error (attempt {attempt + 1})")
                if attempt == self.max_retries - 1:
                    raise
                time.sleep(2 ** attempt)
                
            except RequestException as e:
                if e.response and e.response.status_code >= 500:
                    # æœåŠ¡å™¨é”™è¯¯ï¼Œé‡è¯•
                    self.logger.warning(f"Server error {e.response.status_code} (attempt {attempt + 1})")
                    if attempt == self.max_retries - 1:
                        raise
                    time.sleep(2 ** attempt)
                else:
                    # å®¢æˆ·ç«¯é”™è¯¯ï¼Œä¸é‡è¯•
                    raise
    
    def query_with_fallback(self, query: str, modes: List[str] = None) -> Dict:
        """å¸¦é™çº§ç­–ç•¥çš„æŸ¥è¯¢"""
        if modes is None:
            modes = ["hybrid", "local", "naive"]
        
        last_error = None
        
        for mode in modes:
            try:
                self.logger.info(f"Trying query with mode: {mode}")
                return self._make_request(
                    "POST", "/query",
                    json={"query": query, "mode": mode}
                )
            except Exception as e:
                self.logger.warning(f"Query failed with mode {mode}: {e}")
                last_error = e
                continue
        
        # æ‰€æœ‰æ¨¡å¼éƒ½å¤±è´¥
        raise last_error or Exception("All query modes failed")

# ä½¿ç”¨ç¤ºä¾‹
logging.basicConfig(level=logging.INFO)
robust_client = GuiXiaoXiRagClientWithErrorHandling()

try:
    result = robust_client.query_with_fallback("ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ")
    print("Query successful:", result["data"]["answer"])
except Exception as e:
    print(f"All query attempts failed: {e}")
```

## æ€§èƒ½ä¼˜åŒ–ç¤ºä¾‹

### è¿æ¥æ± å’Œä¼šè¯ç®¡ç†
```python
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

class OptimizedGuiXiaoXiRagClient(GuiXiaoXiRagClient):
    def __init__(self, base_url: str = "http://localhost:8002"):
        super().__init__(base_url)
        
        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        
        # é…ç½®HTTPé€‚é…å™¨
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=10,
            pool_maxsize=20
        )
        
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # è®¾ç½®é»˜è®¤è¶…æ—¶
        self.session.timeout = 30
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.session.close()

# ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨
with OptimizedGuiXiaoXiRagClient() as client:
    # æ‰¹é‡æ“ä½œ
    for i in range(100):
        result = client.query(f"Query {i}")
        print(f"Query {i} completed")
```

### ç¼“å­˜æŸ¥è¯¢ç»“æœ
```python
from functools import lru_cache
import hashlib

class CachedGuiXiaoXiRagClient(GuiXiaoXiRagClient):
    def __init__(self, base_url: str = "http://localhost:8002", cache_size: int = 128):
        super().__init__(base_url)
        self.cache_size = cache_size
    
    def _cache_key(self, query: str, mode: str, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_data = f"{query}:{mode}:{sorted(kwargs.items())}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    @lru_cache(maxsize=128)
    def _cached_query(self, cache_key: str, query: str, mode: str, **kwargs):
        """ç¼“å­˜çš„æŸ¥è¯¢æ–¹æ³•"""
        return super().query(query, mode, **kwargs)
    
    def query(self, query: str, mode: str = "hybrid", **kwargs):
        """å¸¦ç¼“å­˜çš„æŸ¥è¯¢"""
        cache_key = self._cache_key(query, mode, **kwargs)
        return self._cached_query(cache_key, query, mode, **kwargs)

# ä½¿ç”¨ç¼“å­˜å®¢æˆ·ç«¯
cached_client = CachedGuiXiaoXiRagClient()

# ç¬¬ä¸€æ¬¡æŸ¥è¯¢ä¼šè°ƒç”¨API
result1 = cached_client.query("ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ")

# ç¬¬äºŒæ¬¡ç›¸åŒæŸ¥è¯¢ä¼šä½¿ç”¨ç¼“å­˜
result2 = cached_client.query("ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ")  # ä»ç¼“å­˜è¿”å›
```

## çŸ¥è¯†å›¾è°±å¯è§†åŒ–ç¤ºä¾‹

### è·å–å›¾è°±çŠ¶æ€
```python
def check_graph_status(client, knowledge_base="default"):
    """æ£€æŸ¥çŸ¥è¯†å›¾è°±çŠ¶æ€"""
    response = client.session.get(
        f"{client.base_url}/knowledge-graph/status",
        params={"knowledge_base": knowledge_base}
    )

    if response.status_code == 200:
        data = response.json()["data"]
        print(f"GraphMLæ–‡ä»¶å­˜åœ¨: {data['xml_file_exists']}")
        print(f"JSONæ–‡ä»¶å­˜åœ¨: {data['json_file_exists']}")
        print(f"èŠ‚ç‚¹æ•°é‡: {data.get('node_count', 0)}")
        print(f"è¾¹æ•°é‡: {data.get('edge_count', 0)}")

    return response.json()

# ä½¿ç”¨ç¤ºä¾‹
client = GuiXiaoXiRagClient()
status = check_graph_status(client)
```

### ç”Ÿæˆå›¾è°±å¯è§†åŒ–
```python
def generate_visualization(client, knowledge_base="default", max_nodes=50):
    """ç”ŸæˆçŸ¥è¯†å›¾è°±å¯è§†åŒ–"""
    data = {
        "knowledge_base": knowledge_base,
        "max_nodes": max_nodes,
        "layout": "spring",
        "node_size_field": "degree",
        "edge_width_field": "weight"
    }

    response = client.session.post(
        f"{client.base_url}/knowledge-graph/visualize",
        json=data
    )

    if response.status_code == 200:
        result = response.json()["data"]
        print(f"å¯è§†åŒ–HTMLæ–‡ä»¶: {result['html_file']}")
        print(f"å¤„ç†çš„èŠ‚ç‚¹æ•°: {result['processed_nodes']}")
        print(f"å¤„ç†çš„è¾¹æ•°: {result['processed_edges']}")

        # å¯ä»¥åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€HTMLæ–‡ä»¶
        import webbrowser
        webbrowser.open(result['html_file'])

    return response.json()

# ä½¿ç”¨ç¤ºä¾‹
visualization = generate_visualization(client, max_nodes=100)
```

### è·å–å›¾è°±æ•°æ®
```python
def get_graph_data(client, knowledge_base="default", format="json"):
    """è·å–å›¾è°±æ•°æ®"""
    data = {
        "knowledge_base": knowledge_base,
        "format": format
    }

    response = client.session.post(
        f"{client.base_url}/knowledge-graph/data",
        json=data
    )

    if response.status_code == 200:
        result = response.json()["data"]

        if format == "json":
            nodes = result["nodes"]
            edges = result["edges"]
            print(f"è·å–åˆ° {len(nodes)} ä¸ªèŠ‚ç‚¹å’Œ {len(edges)} æ¡è¾¹")

            # åˆ†æèŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ
            node_types = {}
            for node in nodes:
                node_type = node.get("type", "unknown")
                node_types[node_type] = node_types.get(node_type, 0) + 1

            print("èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ:")
            for node_type, count in node_types.items():
                print(f"  {node_type}: {count}")

        return result

    return None

# ä½¿ç”¨ç¤ºä¾‹
graph_data = get_graph_data(client)
```

## é…ç½®ç®¡ç†ç¤ºä¾‹

### è·å–æœ‰æ•ˆé…ç½®
```python
def get_effective_config(client):
    """è·å–å®Œæ•´çš„æœ‰æ•ˆé…ç½®ä¿¡æ¯"""
    response = client.session.get(f"{client.base_url}/service/effective-config")

    if response.status_code == 200:
        result = response.json()["data"]

        print("ğŸ”§ å½“å‰æœ‰æ•ˆé…ç½®:")
        print(f"  åº”ç”¨: {result['app_name']} v{result['version']}")
        print(f"  æœåŠ¡: {result['host']}:{result['port']}")
        print(f"  è°ƒè¯•æ¨¡å¼: {result['debug']}")

        print("\nğŸ§  LLMé…ç½®:")
        llm = result['llm']
        print(f"  æä¾›å•†: {llm['provider']}")
        print(f"  APIåœ°å€: {llm['api_base']}")
        print(f"  æ¨¡å‹: {llm['model']}")
        print(f"  APIå¯†é’¥: {llm['api_key']}")

        print("\nğŸ“Š Embeddingé…ç½®:")
        embedding = result['embedding']
        print(f"  æä¾›å•†: {embedding['provider']}")
        print(f"  APIåœ°å€: {embedding['api_base']}")
        print(f"  APIå¯†é’¥: {embedding['api_key']}")
        print(f"  æ¨¡å‹: {embedding['model']}")
        print(f"  ç»´åº¦: {embedding['dim']}")

        print("\nâš™ï¸ å…¶ä»–é…ç½®:")
        print(f"  å·¥ä½œç›®å½•: {result['working_dir']}")
        print(f"  æ—¥å¿—çº§åˆ«: {result['log_level']}")
        print(f"  æœ€å¤§æ–‡ä»¶å¤§å°: {result['max_file_size_mb']}MB")
        print(f"  æœ€å¤§Tokenæ•°: {result['max_token_size']}")

        # æ£€æŸ¥æ˜¯å¦æœ‰Azureé…ç½®
        if 'azure' in result:
            print("\nâ˜ï¸ Azureé…ç½®:")
            azure = result['azure']
            print(f"  APIç‰ˆæœ¬: {azure['api_version']}")
            print(f"  éƒ¨ç½²åç§°: {azure['deployment_name']}")

        return result

    return None

# ä½¿ç”¨ç¤ºä¾‹
client = GuiXiaoXiRagClient()
config = get_effective_config(client)
```

### é…ç½®éªŒè¯å’Œè¯Šæ–­
```python
def validate_config(client):
    """éªŒè¯é…ç½®å¹¶æä¾›è¯Šæ–­ä¿¡æ¯"""
    config = get_effective_config(client)
    if not config:
        print("âŒ æ— æ³•è·å–é…ç½®ä¿¡æ¯")
        return False

    issues = []
    warnings = []

    # æ£€æŸ¥LLMé…ç½®
    llm = config['llm']
    if llm['api_key'] == "æœªé…ç½®":
        issues.append("LLM APIå¯†é’¥æœªé…ç½®")

    if not llm['api_base'].startswith(('http://', 'https://')):
        issues.append(f"LLM APIåœ°å€æ ¼å¼æ— æ•ˆ: {llm['api_base']}")

    # æ£€æŸ¥Embeddingé…ç½®
    embedding = config['embedding']
    if embedding['api_key'] == "æœªé…ç½®":
        issues.append("Embedding APIå¯†é’¥æœªé…ç½®")

    if not embedding['api_base'].startswith(('http://', 'https://')):
        issues.append(f"Embedding APIåœ°å€æ ¼å¼æ— æ•ˆ: {embedding['api_base']}")

    # æ£€æŸ¥ç«¯å£é…ç½®
    if config['port'] == config.get('streamlit_port'):
        warnings.append("APIæœåŠ¡ç«¯å£ä¸Streamlitç«¯å£ç›¸åŒï¼Œå¯èƒ½å¯¼è‡´å†²çª")

    # è¾“å‡ºè¯Šæ–­ç»“æœ
    if issues:
        print("âŒ é…ç½®é—®é¢˜:")
        for issue in issues:
            print(f"  - {issue}")

    if warnings:
        print("âš ï¸ é…ç½®è­¦å‘Š:")
        for warning in warnings:
            print(f"  - {warning}")

    if not issues and not warnings:
        print("âœ… é…ç½®éªŒè¯é€šè¿‡")

    return len(issues) == 0

# ä½¿ç”¨ç¤ºä¾‹
is_valid = validate_config(client)
```

## é…ç½®æ›´æ–°ç¤ºä¾‹

### åŠ¨æ€æ›´æ–°é…ç½®
```python
def update_service_config(client, **config_updates):
    """åŠ¨æ€æ›´æ–°æœåŠ¡é…ç½®"""
    response = client.session.post(
        f"{client.base_url}/service/config/update",
        json=config_updates
    )

    if response.status_code == 200:
        result = response.json()["data"]

        print("âœ… é…ç½®æ›´æ–°æˆåŠŸ:")
        print(f"  æ›´æ–°å­—æ®µ: {', '.join(result['updated_fields'])}")
        print(f"  éœ€è¦é‡å¯: {'æ˜¯' if result['restart_required'] else 'å¦'}")

        # æ˜¾ç¤ºæ›´æ–°åçš„é…ç½®
        effective_config = result['effective_config']
        if 'llm' in effective_config:
            llm = effective_config['llm']
            print(f"\nğŸ§  LLMé…ç½®:")
            print(f"  æ¨¡å‹: {llm.get('model', 'N/A')}")
            print(f"  APIåœ°å€: {llm.get('api_base', 'N/A')}")

        if 'embedding' in effective_config:
            embedding = effective_config['embedding']
            print(f"\nğŸ“Š Embeddingé…ç½®:")
            print(f"  æ¨¡å‹: {embedding.get('model', 'N/A')}")
            print(f"  ç»´åº¦: {embedding.get('dim', 'N/A')}")

        return result
    else:
        print(f"âŒ é…ç½®æ›´æ–°å¤±è´¥: HTTP {response.status_code}")
        return None

# ä½¿ç”¨ç¤ºä¾‹
client = GuiXiaoXiRagClient()

# æ›´æ–°LLMæ¨¡å‹
update_service_config(client,
    openai_chat_model="gpt-4",
    log_level="DEBUG"
)

# æ›´æ–°APIå¯†é’¥
update_service_config(client,
    openai_chat_api_key="new_llm_key",
    openai_embedding_api_key="new_embedding_key"
)

# æ›´æ–°Azureé…ç½®
update_service_config(client,
    custom_llm_provider="azure",
    azure_api_version="2023-12-01-preview",
    azure_deployment_name="my-deployment"
)
```

## æ–‡ä»¶ä¸Šä¼ ç¤ºä¾‹

### å•æ–‡ä»¶ä¸Šä¼ åˆ°æŒ‡å®šçŸ¥è¯†åº“
```python
def upload_file_to_kb(client, file_path, knowledge_base="default", language="ä¸­æ–‡"):
    """ä¸Šä¼ æ–‡ä»¶åˆ°æŒ‡å®šçŸ¥è¯†åº“"""
    with open(file_path, "rb") as f:
        files = {"file": f}
        data = {
            "knowledge_base": knowledge_base,
            "language": language,
            "track_id": f"upload_{int(time.time())}"
        }

        response = client.session.post(
            f"{client.base_url}/insert/file",
            files=files,
            data=data
        )

    if response.status_code == 200:
        result = response.json()["data"]
        print(f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸ:")
        print(f"  æ–‡ä»¶å: {result['filename']}")
        print(f"  æ–‡ä»¶å¤§å°: {result['file_size']} bytes")
        print(f"  çŸ¥è¯†åº“: {result['knowledge_base']}")
        print(f"  è¯­è¨€: {result['language']}")
        print(f"  è·Ÿè¸ªID: {result['track_id']}")
        return result
    else:
        print(f"âŒ æ–‡ä»¶ä¸Šä¼ å¤±è´¥: HTTP {response.status_code}")
        return None

# ä½¿ç”¨ç¤ºä¾‹
result = upload_file_to_kb(client, "document.pdf", "my_knowledge_base", "ä¸­æ–‡")
```

### æ‰¹é‡æ–‡ä»¶ä¸Šä¼ 
```python
def upload_multiple_files(client, file_paths, knowledge_base="default", language="ä¸­æ–‡"):
    """æ‰¹é‡ä¸Šä¼ æ–‡ä»¶åˆ°æŒ‡å®šçŸ¥è¯†åº“"""
    files = []
    try:
        # å‡†å¤‡æ–‡ä»¶
        for file_path in file_paths:
            files.append(("files", open(file_path, "rb")))

        data = {
            "knowledge_base": knowledge_base,
            "language": language,
            "track_id": f"batch_upload_{int(time.time())}"
        }

        response = client.session.post(
            f"{client.base_url}/insert/files",
            files=files,
            data=data
        )

        if response.status_code == 200:
            result = response.json()["data"]
            print(f"âœ… æ‰¹é‡ä¸Šä¼ æˆåŠŸ:")
            print(f"  æ–‡ä»¶æ•°é‡: {result['total_files']}")
            print(f"  çŸ¥è¯†åº“: {result['knowledge_base']}")
            print(f"  è¯­è¨€: {result['language']}")
            print(f"  è·Ÿè¸ªID: {result['track_id']}")

            print("\nğŸ“ ä¸Šä¼ çš„æ–‡ä»¶:")
            for file_info in result['files']:
                print(f"  - {file_info['filename']} ({file_info['file_size']} bytes)")

            return result
        else:
            print(f"âŒ æ‰¹é‡ä¸Šä¼ å¤±è´¥: HTTP {response.status_code}")
            return None

    finally:
        # å…³é—­æ–‡ä»¶
        for _, file_obj in files:
            file_obj.close()

# ä½¿ç”¨ç¤ºä¾‹
file_list = ["doc1.pdf", "doc2.docx", "doc3.txt"]
result = upload_multiple_files(client, file_list, "project_docs", "ä¸­æ–‡")
```

### é…ç½®å’Œæ–‡ä»¶ä¸Šä¼ ç»„åˆç¤ºä¾‹
```python
def setup_and_upload(client, config_updates, file_paths, knowledge_base):
    """é…ç½®æ›´æ–°åä¸Šä¼ æ–‡ä»¶"""
    # 1. æ›´æ–°é…ç½®
    print("ğŸ”§ æ›´æ–°é…ç½®...")
    config_result = update_service_config(client, **config_updates)

    if not config_result:
        print("âŒ é…ç½®æ›´æ–°å¤±è´¥ï¼Œåœæ­¢ä¸Šä¼ ")
        return False

    # 2. å¦‚æœéœ€è¦é‡å¯ï¼Œæç¤ºç”¨æˆ·
    if config_result['restart_required']:
        print("âš ï¸ é…ç½®æ›´æ”¹éœ€è¦é‡å¯æœåŠ¡æ‰èƒ½å®Œå…¨ç”Ÿæ•ˆ")

    # 3. ä¸Šä¼ æ–‡ä»¶
    print(f"\nğŸ“ ä¸Šä¼ æ–‡ä»¶åˆ°çŸ¥è¯†åº“ '{knowledge_base}'...")
    upload_result = upload_multiple_files(client, file_paths, knowledge_base)

    return upload_result is not None

# ä½¿ç”¨ç¤ºä¾‹
config_updates = {
    "openai_chat_model": "gpt-4",
    "embedding_dim": 1536,
    "log_level": "INFO"
}

file_paths = ["research_paper.pdf", "notes.txt"]

success = setup_and_upload(client, config_updates, file_paths, "research_kb")
print(f"\n{'âœ… æ“ä½œå®Œæˆ' if success else 'âŒ æ“ä½œå¤±è´¥'}")
```

## ç¼“å­˜ç®¡ç†ç¤ºä¾‹

### è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
```python
def get_cache_statistics(client):
    """è·å–è¯¦ç»†çš„ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    response = client.session.get(f"{client.base_url}/cache/stats")

    if response.status_code == 200:
        data = response.json()["data"]

        print("ğŸ“Š ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  è¿›ç¨‹å†…å­˜: {data['total_memory_mb']:.2f} MB")

        # ç³»ç»Ÿå†…å­˜ä¿¡æ¯
        system_memory = data.get('system_memory', {})
        print(f"\nğŸ’¾ ç³»ç»Ÿå†…å­˜:")
        print(f"  æ€»å†…å­˜: {system_memory.get('total_mb', 0):.1f} MB")
        print(f"  å¯ç”¨å†…å­˜: {system_memory.get('available_mb', 0):.1f} MB")
        print(f"  ä½¿ç”¨ç‡: {system_memory.get('used_percent', 0):.1f}%")

        # å„ç±»ç¼“å­˜è¯¦æƒ…
        caches = data.get('caches', {})
        if caches:
            print(f"\nğŸ“¦ ç¼“å­˜è¯¦æƒ…:")
            for cache_name, cache_info in caches.items():
                print(f"  {cache_name.upper()}:")
                print(f"    é¡¹ç›®æ•°: {cache_info.get('item_count', 0)}")
                print(f"    å¤§å°: {cache_info.get('size_mb', 0):.2f} MB")
                print(f"    å‘½ä¸­ç‡: {cache_info.get('hit_rate', 0):.1%}")

        return data
    else:
        print(f"âŒ è·å–ç¼“å­˜ç»Ÿè®¡å¤±è´¥: HTTP {response.status_code}")
        return None

# ä½¿ç”¨ç¤ºä¾‹
client = GuiXiaoXiRagClient()
cache_stats = get_cache_statistics(client)
```

### æ¸…ç†æ‰€æœ‰ç¼“å­˜
```python
def clear_all_caches(client):
    """æ¸…ç†æ‰€æœ‰ç³»ç»Ÿç¼“å­˜"""
    print("ğŸ—‘ï¸ å¼€å§‹æ¸…ç†æ‰€æœ‰ç¼“å­˜...")

    response = client.session.delete(f"{client.base_url}/cache/clear")

    if response.status_code == 200:
        data = response.json()["data"]

        print("âœ… ç¼“å­˜æ¸…ç†æˆåŠŸ!")
        print(f"  é‡Šæ”¾å†…å­˜: {data.get('freed_memory_mb', 0):.2f} MB")
        print(f"  åƒåœ¾å›æ”¶å¯¹è±¡: {data.get('gc_collected_objects', 0)}")

        cleared_caches = data.get('cleared_caches', [])
        if cleared_caches:
            print("  æ¸…ç†çš„ç¼“å­˜ç±»å‹:")
            for cache in cleared_caches:
                print(f"    - {cache}")

        # æ˜¾ç¤ºæ¸…ç†å‰åå¯¹æ¯”
        cache_stats = data.get('cache_stats', {})
        if cache_stats:
            before = cache_stats.get('before', {})
            after = cache_stats.get('after', {})
            print(f"\nğŸ“Š å†…å­˜å˜åŒ–:")
            print(f"  æ¸…ç†å‰: {before.get('memory_mb', 0):.2f} MB")
            print(f"  æ¸…ç†å: {after.get('memory_mb', 0):.2f} MB")

        return True
    else:
        print(f"âŒ æ¸…ç†ç¼“å­˜å¤±è´¥: HTTP {response.status_code}")
        return False

# ä½¿ç”¨ç¤ºä¾‹
success = clear_all_caches(client)
```

### æ¸…ç†æŒ‡å®šç±»å‹ç¼“å­˜
```python
def clear_specific_cache_type(client, cache_type):
    """æ¸…ç†æŒ‡å®šç±»å‹çš„ç¼“å­˜"""
    supported_types = ["llm", "vector", "knowledge_graph", "documents", "queries"]

    if cache_type not in supported_types:
        print(f"âŒ ä¸æ”¯æŒçš„ç¼“å­˜ç±»å‹: {cache_type}")
        print(f"   æ”¯æŒçš„ç±»å‹: {', '.join(supported_types)}")
        return False

    print(f"ğŸ—‘ï¸ å¼€å§‹æ¸…ç† {cache_type.upper()} ç¼“å­˜...")

    response = client.session.delete(f"{client.base_url}/cache/clear/{cache_type}")

    if response.status_code == 200:
        data = response.json()["data"]

        print(f"âœ… {cache_type.upper()} ç¼“å­˜æ¸…ç†æˆåŠŸ!")
        print(f"  æ¸…ç†é¡¹ç›®æ•°: {data.get('cleared_items', 0)}")
        print(f"  é‡Šæ”¾å†…å­˜: {data.get('freed_memory_mb', 0):.2f} MB")
        print(f"  åƒåœ¾å›æ”¶å¯¹è±¡: {data.get('gc_collected_objects', 0)}")

        return True
    else:
        print(f"âŒ æ¸…ç† {cache_type.upper()} ç¼“å­˜å¤±è´¥: HTTP {response.status_code}")
        return False

# ä½¿ç”¨ç¤ºä¾‹
# æ¸…ç†LLMç¼“å­˜
clear_specific_cache_type(client, "llm")

# æ¸…ç†å‘é‡ç¼“å­˜
clear_specific_cache_type(client, "vector")

# æ¸…ç†çŸ¥è¯†å›¾è°±ç¼“å­˜
clear_specific_cache_type(client, "knowledge_graph")
```

### ç¼“å­˜ç®¡ç†å·¥å…·ç±»
```python
class CacheManager:
    """ç¼“å­˜ç®¡ç†å·¥å…·ç±»"""

    def __init__(self, client):
        self.client = client
        self.base_url = client.base_url

    def get_stats(self):
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        response = self.client.session.get(f"{self.base_url}/cache/stats")
        return response.json()["data"] if response.status_code == 200 else None

    def clear_all(self):
        """æ¸…ç†æ‰€æœ‰ç¼“å­˜"""
        response = self.client.session.delete(f"{self.base_url}/cache/clear")
        return response.json()["data"] if response.status_code == 200 else None

    def clear_type(self, cache_type):
        """æ¸…ç†æŒ‡å®šç±»å‹ç¼“å­˜"""
        response = self.client.session.delete(f"{self.base_url}/cache/clear/{cache_type}")
        return response.json()["data"] if response.status_code == 200 else None

    def monitor_memory(self, interval=60, duration=300):
        """ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        import time

        start_time = time.time()
        memory_history = []

        print(f"ğŸ” å¼€å§‹ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ (é—´éš”: {interval}s, æŒç»­: {duration}s)")

        while time.time() - start_time < duration:
            stats = self.get_stats()
            if stats:
                timestamp = time.strftime("%H:%M:%S")
                memory_mb = stats.get('total_memory_mb', 0)
                memory_history.append({
                    'time': timestamp,
                    'memory_mb': memory_mb
                })

                print(f"  {timestamp}: {memory_mb:.2f} MB")

            time.sleep(interval)

        return memory_history

    def auto_cleanup(self, memory_threshold_mb=1000):
        """è‡ªåŠ¨æ¸…ç†ç¼“å­˜ï¼ˆå½“å†…å­˜è¶…è¿‡é˜ˆå€¼æ—¶ï¼‰"""
        stats = self.get_stats()
        if not stats:
            return False

        current_memory = stats.get('total_memory_mb', 0)

        if current_memory > memory_threshold_mb:
            print(f"âš ï¸ å†…å­˜ä½¿ç”¨è¶…è¿‡é˜ˆå€¼ ({current_memory:.2f} MB > {memory_threshold_mb} MB)")
            print("ğŸ—‘ï¸ å¼€å§‹è‡ªåŠ¨æ¸…ç†ç¼“å­˜...")

            # æŒ‰ä¼˜å…ˆçº§æ¸…ç†ç¼“å­˜
            cleanup_order = ["queries", "documents", "llm", "vector"]

            for cache_type in cleanup_order:
                result = self.clear_type(cache_type)
                if result:
                    print(f"  âœ… æ¸…ç† {cache_type.upper()} ç¼“å­˜æˆåŠŸ")

                    # é‡æ–°æ£€æŸ¥å†…å­˜
                    new_stats = self.get_stats()
                    if new_stats:
                        new_memory = new_stats.get('total_memory_mb', 0)
                        if new_memory <= memory_threshold_mb:
                            print(f"  âœ… å†…å­˜å·²é™è‡³å®‰å…¨æ°´å¹³: {new_memory:.2f} MB")
                            return True

            return False
        else:
            print(f"âœ… å†…å­˜ä½¿ç”¨æ­£å¸¸: {current_memory:.2f} MB")
            return True

# ä½¿ç”¨ç¤ºä¾‹
cache_manager = CacheManager(client)

# è·å–ç»Ÿè®¡ä¿¡æ¯
stats = cache_manager.get_stats()

# ç›‘æ§å†…å­˜ä½¿ç”¨
memory_history = cache_manager.monitor_memory(interval=30, duration=180)

# è‡ªåŠ¨æ¸…ç†
cache_manager.auto_cleanup(memory_threshold_mb=800)
```

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [APIå‚è€ƒæ–‡æ¡£](API_REFERENCE.md)
- [å¿«é€Ÿå¼€å§‹æŒ‡å—](../getting-started/QUICK_START.md)
- [é…ç½®æŒ‡å—](../getting-started/CONFIGURATION_GUIDE.md)
- [æ•…éšœæ’é™¤æŒ‡å—](../getting-started/TROUBLESHOOTING.md)
- [Pythonå®¢æˆ·ç«¯ç¤ºä¾‹](../../examples/api_client.py)
