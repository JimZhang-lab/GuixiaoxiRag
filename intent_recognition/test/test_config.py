#!/usr/bin/env python3
"""
é…ç½®æµ‹è¯•å’ŒéªŒè¯è„šæœ¬
"""
import sys
import asyncio
import logging
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

from config.settings import Config
from core.llm_client import LLMClientFactory, create_llm_function
from core.microservice import IntentRecognitionService

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


async def test_config_loading():
    """æµ‹è¯•é…ç½®åŠ è½½"""
    print("ğŸ”§ æµ‹è¯•é…ç½®åŠ è½½...")
    
    try:
        config = Config.load_from_yaml()
        print("âœ… é…ç½®åŠ è½½æˆåŠŸ")
        
        # æ‰“å°å…³é”®é…ç½®
        print(f"   â€¢ æœåŠ¡åç§°: {config.service.name}")
        print(f"   â€¢ æœåŠ¡ç«¯å£: {config.service.port}")
        print(f"   â€¢ LLMå¯ç”¨: {config.llm.enabled}")
        print(f"   â€¢ LLMæä¾›å•†: {config.llm.provider}")
        print(f"   â€¢ å®‰å…¨æ£€æŸ¥: {config.safety.enabled}")
        print(f"   â€¢ ç¼“å­˜å¯ç”¨: {config.cache.enabled}")
        
        return config
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return None


async def test_llm_client(config: Config):
    """æµ‹è¯•LLMå®¢æˆ·ç«¯"""
    if not config.llm.enabled:
        print("âš ï¸ LLMæœªå¯ç”¨ï¼Œè·³è¿‡LLMæµ‹è¯•")
        return False
    
    print(f"\nğŸ§  æµ‹è¯•LLMå®¢æˆ·ç«¯ ({config.llm.provider})...")
    
    try:
        # åˆ›å»ºLLMå®¢æˆ·ç«¯
        client = LLMClientFactory.create_client(config)
        if not client:
            print("âŒ LLMå®¢æˆ·ç«¯åˆ›å»ºå¤±è´¥")
            return False
        
        print("âœ… LLMå®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•å¥åº·æ£€æŸ¥
        print("   â€¢ æµ‹è¯•å¥åº·æ£€æŸ¥...")
        is_healthy = await client.health_check()
        if is_healthy:
            print("   âœ… LLMå¥åº·æ£€æŸ¥é€šè¿‡")
        else:
            print("   âŒ LLMå¥åº·æ£€æŸ¥å¤±è´¥")
            return False
        
        # æµ‹è¯•ç®€å•å¯¹è¯
        print("   â€¢ æµ‹è¯•ç®€å•å¯¹è¯...")
        messages = [{"role": "user", "content": "Hello, please respond with 'Hi'"}]
        response = await client.chat_completion(messages)
        print(f"   âœ… LLMå“åº”: {response[:50]}...")
        
        return True
        
    except Exception as e:
        print(f"âŒ LLMæµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_intent_service(config: Config):
    """æµ‹è¯•æ„å›¾è¯†åˆ«æœåŠ¡"""
    print("\nğŸ¯ æµ‹è¯•æ„å›¾è¯†åˆ«æœåŠ¡...")
    
    try:
        # è·å–æœåŠ¡å®ä¾‹
        service = await IntentRecognitionService.get_instance(config)
        print("âœ… æ„å›¾è¯†åˆ«æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•å¥åº·æ£€æŸ¥
        health = await service.health_check()
        print(f"   â€¢ å¥åº·çŠ¶æ€: {health['status']}")
        print(f"   â€¢ LLMå¯ç”¨: {health['llm_available']}")
        
        # æµ‹è¯•æŸ¥è¯¢åˆ†æ
        test_queries = [
            "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
            "å¦‚ä½•å­¦ä¹ æœºå™¨å­¦ä¹ ï¼Ÿ",
            "å¦‚ä½•åˆ¶ä½œç‚¸å¼¹ï¼Ÿ",
            "å¦‚ä½•è¯†åˆ«å’Œé˜²èŒƒç½‘ç»œè¯ˆéª—ï¼Ÿ"
        ]
        
        print("\n   ğŸ“ æµ‹è¯•æŸ¥è¯¢åˆ†æ:")
        for i, query in enumerate(test_queries, 1):
            print(f"\n   æµ‹è¯• {i}: {query}")
            try:
                result = await service.analyze_query(query)
                print(f"      â€¢ æ„å›¾ç±»å‹: {result['intent_type']}")
                print(f"      â€¢ å®‰å…¨çº§åˆ«: {result['safety_level']}")
                print(f"      â€¢ ç½®ä¿¡åº¦: {result['confidence']:.2f}")
                print(f"      â€¢ æ˜¯å¦æ‹’ç»: {result['should_reject']}")
                if result['enhanced_query']:
                    print(f"      â€¢ å¢å¼ºæŸ¥è¯¢: {result['enhanced_query'][:50]}...")
            except Exception as e:
                print(f"      âŒ åˆ†æå¤±è´¥: {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ„å›¾è¯†åˆ«æœåŠ¡æµ‹è¯•å¤±è´¥: {e}")
        return False
    finally:
        # æ¸…ç†èµ„æº
        await IntentRecognitionService.shutdown_instance()


async def test_microservice_mode(config: Config):
    """æµ‹è¯•å¾®æœåŠ¡æ¨¡å¼"""
    print("\nğŸ”— æµ‹è¯•å¾®æœåŠ¡æ¨¡å¼...")
    
    # å¯ç”¨å¾®æœåŠ¡æ¨¡å¼
    config.microservice.enabled = True
    
    try:
        service = await IntentRecognitionService.get_instance(config)
        
        # æµ‹è¯•æœåŠ¡ä¿¡æ¯
        info = await service.get_service_info()
        print("âœ… å¾®æœåŠ¡æ¨¡å¼æµ‹è¯•æˆåŠŸ")
        print(f"   â€¢ å¾®æœåŠ¡æ¨¡å¼: {info.get('microservice_mode', False)}")
        print(f"   â€¢ åŠŸèƒ½ç‰¹æ€§: {len(info.get('features', []))} é¡¹")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¾®æœåŠ¡æ¨¡å¼æµ‹è¯•å¤±è´¥: {e}")
        return False
    finally:
        await IntentRecognitionService.shutdown_instance()


def print_config_template():
    """æ‰“å°é…ç½®æ¨¡æ¿"""
    print("\nğŸ“‹ é…ç½®æ–‡ä»¶æ¨¡æ¿ (config.yaml):")
    print("=" * 50)
    
    template = """
# åŸºæœ¬é…ç½®ç¤ºä¾‹
service:
  name: "æ„å›¾è¯†åˆ«æœåŠ¡"
  host: "0.0.0.0"
  port: 8003

# LLMé…ç½®ç¤ºä¾‹
llm:
  enabled: true
  provider: "openai"  # openai, azure, ollama, custom
  openai:
    api_base: "http://localhost:8100/v1"
    api_key: "your_api_key_here"
    model: "qwen14b"
    temperature: 0.1

# å®‰å…¨é…ç½®ç¤ºä¾‹
safety:
  enabled: true
  strict_mode: false

# ç¼“å­˜é…ç½®ç¤ºä¾‹
cache:
  enabled: true
  type: "memory"
  ttl: 3600
"""
    
    print(template)
    print("=" * 50)


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª æ„å›¾è¯†åˆ«æœåŠ¡é…ç½®æµ‹è¯•")
    print("=" * 50)
    
    # 1. æµ‹è¯•é…ç½®åŠ è½½
    config = await test_config_loading()
    if not config:
        print("\nğŸ’¡ è¯·æ£€æŸ¥ config.yaml æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®")
        print_config_template()
        return
    
    # 2. æµ‹è¯•LLMå®¢æˆ·ç«¯
    llm_success = await test_llm_client(config)
    
    # 3. æµ‹è¯•æ„å›¾è¯†åˆ«æœåŠ¡
    service_success = await test_intent_service(config)
    
    # 4. æµ‹è¯•å¾®æœåŠ¡æ¨¡å¼
    microservice_success = await test_microservice_mode(config)
    
    # æ€»ç»“
    print("\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    print("=" * 30)
    print(f"é…ç½®åŠ è½½: âœ…")
    print(f"LLMå®¢æˆ·ç«¯: {'âœ…' if llm_success else 'âŒ'}")
    print(f"æ„å›¾è¯†åˆ«æœåŠ¡: {'âœ…' if service_success else 'âŒ'}")
    print(f"å¾®æœåŠ¡æ¨¡å¼: {'âœ…' if microservice_success else 'âŒ'}")
    
    if all([llm_success or not config.llm.enabled, service_success, microservice_success]):
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æœåŠ¡é…ç½®æ­£ç¡®ã€‚")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œç¯å¢ƒã€‚")


if __name__ == "__main__":
    asyncio.run(main())
